# EVE - Evolutionary Intelligence Through Bit Evolution

**Un modello di AI rivoluzionario basato su evoluzione biologica invece che gradient descent.**

## ğŸ§¬ Filosofia

EVE risponde a una domanda fondamentale:

> "Se i computer a livello hardware usano SOLO operatori booleani, perchÃ© non lavorare direttamente su quel substrato invece di attraverso layer di astrazione (floating point, backprop, GPU)?"

EVE elimina:
- âŒ Floating point arithmetic
- âŒ Backpropagation
- âŒ Gradient descent
- âŒ Layer feedforward

E usa invece:
- âœ… Operatori booleani discreti (NOT, XOR, AND, shift, majority)
- âœ… Evoluzione darwiniana (mutazione + selezione)
- âœ… Propagazione spaziale attraverso lattice toroidale
- âœ… Computazione emergente da regole semplici

## ğŸ—ï¸ Architettura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   EVE SYSTEM                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚   Dataset    â”‚â”€â”€â”€â”€â”€>â”‚ BitStream    â”‚           â”‚
â”‚  â”‚   Loader     â”‚      â”‚  Converter   â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚         â”‚                      â”‚                   â”‚
â”‚         v                      v                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚    Lattice Map (D-dimensional)   â”‚             â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”      â”‚             â”‚
â”‚  â”‚  â”‚OP â”‚OP â”‚OP â”‚OP â”‚OP â”‚OP â”‚      â”‚             â”‚
â”‚  â”‚  â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤      â”‚             â”‚
â”‚  â”‚  â”‚OP â”‚OP â”‚OP â”‚OP â”‚OP â”‚OP â”‚      â”‚  Operators: â”‚
â”‚  â”‚  â”œâ”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¤      â”‚  - NOT      â”‚
â”‚  â”‚  â”‚OP â”‚OP â”‚OP â”‚OP â”‚OP â”‚OP â”‚      â”‚  - XOR      â”‚
â”‚  â”‚  â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜      â”‚  - AND      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - SHIFT    â”‚
â”‚         â”‚                               - MAJORITY â”‚
â”‚         v                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚   Beam Search Propagation        â”‚             â”‚
â”‚  â”‚   (max_steps, beam_width)        â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚         â”‚                                          â”‚
â”‚         v                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚  Evolutionary Selection          â”‚             â”‚
â”‚  â”‚  - Fitness evaluation            â”‚             â”‚
â”‚  â”‚  - Elite preservation            â”‚             â”‚
â”‚  â”‚  - Mutation & reproduction       â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Struttura Progetto

```
EVE/
â”œâ”€â”€ brain.py              # Lattice map, operatori, propagazione
â”œâ”€â”€ interace.py           # Translators (text/bytes â†” bits)
â”œâ”€â”€ executor.py           # Sistema evolutivo base
â”œâ”€â”€ config.py             # Sistema configurazione
â”œâ”€â”€ reward_system.py      # Reward predittivo + metriche
â”œâ”€â”€ data_loader.py        # Caricamento dataset universale
â”œâ”€â”€ train.py              # Training evolutivo
â”œâ”€â”€ inference.py          # Chat e inferenza
â”œâ”€â”€ tui.py                # Text User Interface
â””â”€â”€ README.md             # Questo file
```

## ğŸš€ Quick Start

### 1. Avvia la TUI

```bash
python tui.py
```

### 2. Configura Settings (opzionale)

Menu: `3. Configure settings`

Imposta:
- **Lattice**: dimensioni, beam width, max steps
- **Evolution**: population size, mutation rate, generations
- **Training**: mask ratio, batch size
- **Inference**: max response length, context window

### 3. Train un Modello

Menu: `1. Train new model`

- Fornisci path a dataset (file .txt, .md, .json, .csv o directory)
- Il sistema:
  1. Carica il dataset
  2. Converte testo in bit
  3. Maschera bit random (es. 30%)
  4. Evolve popolazione di lattice per predire bit mascherati
  5. Salva checkpoint in `./checkpoints/`

**Esempio:**

```bash
Dataset path: ./my_dataset.txt
Generations [100]: 50
```

### 4. Chat con Modello Trained

Menu: `2. Chat with trained model`

- Seleziona checkpoint da `./checkpoints/`
- Inizia conversazione
- Scrivi `exit` o `quit` per terminare

## ğŸ¯ Come Funziona il Training

### Predictive Training

1. **Carica dataset** (qualsiasi testo/dati)
2. **Converti in bit stream** (UTF-8 encoding)
3. **Maschera bit random** (es. 30%)
   ```
   Original: [1,0,1,1,0,1,0,0,1,1,...]
   Masked:   [1,0,0,0,0,1,0,0,0,0,...]
              Mask:    [0,0,1,1,0,0,0,0,1,1,...]
   ```
4. **Propaga attraverso lattice**
   - Input bits â†’ lattice â†’ operatori booleani â†’ output bits
5. **Valuta fitness** (accuracy su bit mascherati)
6. **Evoluzione**
   - Preserva elite (top 25%)
   - Muta e riproduci
   - Nuova generazione

### Metriche

- **Accuracy**: % bit corretti
- **Precision**: vero positivi / (vero positivi + falsi positivi)
- **Recall**: vero positivi / (vero positivi + falsi negativi)
- **F1 Score**: media armonica precision/recall
- **Hamming Distance**: distanza bit-level

## ğŸ’¡ Esempi di Uso

### Training su Codice Sorgente

```bash
# Carica tutto il codice Python da una directory
Dataset path: ./my_project/
Mask ratio: 0.25
Generations: 100
```

### Training su Letteratura

```bash
# Carica romanzo o documenti
Dataset path: ./books/
Mask ratio: 0.30
Generations: 200
```

### Training su Dati Strutturati

```bash
# JSON, CSV, etc
Dataset path: ./data.json
Mask ratio: 0.20
Generations: 150
```

### Training su Wikipedia

```bash
# Dump XML di Wikipedia (anche compressi!)
Dataset path: ./itwiki-latest-pages-articles.xml.bz2
# oppure
Dataset path: ./enwiki-latest-pages-articles.xml.gz
Mask ratio: 0.25
Generations: 500
```

**Formati supportati:**
- `.xml` - XML semplice
- `.xml.gz` - XML compresso gzip
- `.xml.bz2` - XML compresso bzip2 (formato standard Wikipedia)
- Qualsiasi file con "wiki" nel nome viene automaticamente parsato come dump Wikipedia

## âš™ï¸ Configurazione Avanzata

### File `eve_config.json`

```json
{
  "lattice": {
    "dimensions": 2,
    "size_per_dim": 8,
    "beam_width": 4,
    "max_steps": 16
  },
  "evolution": {
    "population_size": 32,
    "elite_ratio": 0.25,
    "mutation_rate": 0.1,
    "generations": 100
  },
  "training": {
    "mask_ratio": 0.3,
    "batch_size": 16,
    "save_interval": 10,
    "checkpoint_dir": "./checkpoints"
  },
  "inference": {
    "max_response_length": 512,
    "temperature": 1.0,
    "context_window": 1024
  }
}
```

Salva/carica con TUI (opzioni 5 e 6).

## ğŸ”¬ Approccio Scientifico

### Domande di Ricerca

1. **PuÃ² l'evoluzione competere con gradient descent?**
   - VelocitÃ  di convergenza
   - Sample efficiency
   - Generalizzazione

2. **Efficienza computazionale?**
   - Operazioni booleane vs float
   - Energia consumata
   - Throughput

3. **EspressivitÃ  del substrato?**
   - Quali task puÃ² apprendere?
   - Limiti teorici
   - Scaling behavior

### Confronto con Neural Networks

| Metrica | Neural Networks | EVE |
|---------|----------------|-----|
| Operazioni base | Float multiply-add | Boolean ops |
| Ottimizzazione | Gradient descent | Evolution |
| InterpretabilitÃ  | Bassa (black box) | Alta (trace bit-by-bit) |
| Hardware | GPU (float heavy) | FPGA/ASIC (bool native) |
| Energia/op | Alta | Potenzialmente bassa |
| Training parallelization | Batch gradients | Population evaluation |

## ğŸ¨ Estensioni Possibili

### 1. Operatori Personalizzati

Aggiungi nuovi operatori in `brain.py`:

```python
def op_custom(state, neighbors):
    # Tua logica booleana
    return new_state
```

### 2. Reward Multi-Obiettivo

Modifica `reward_system.py`:

```python
fitness = (
    0.5 * accuracy +
    0.3 * diversity +
    0.2 * efficiency
)
```

### 3. Adaptive Difficulty

Aumenta `mask_ratio` durante training se accuracy > threshold.

### 4. Hierarchical Lattices

Lattice nested per processare informazione a scale diverse.

## ğŸ› Troubleshooting

### "No data loaded"

Verifica che il path del dataset sia corretto e contenga file supportati (.txt, .json, .csv, .md).

### "Training very slow"

Riduci:
- `population_size`
- `lattice.size_per_dim`
- `batch_size`

### "Output random/garbage"

Normale per modelli non trained. Serve training sostanziale (100+ generazioni) su dataset consistente.

### "Memory error"

Riduci:
- `chunk_size` in DataLoader
- `context_window` in inference
- `population_size`

## ğŸ“Š Benchmark (Da Fare)

TODO: Aggiungere benchmark su:
- [ ] Text reconstruction accuracy
- [ ] Training time vs NN equivalente
- [ ] Energy consumption
- [ ] Generalization su unseen data
- [ ] Scaling laws (population size, lattice size)

## ğŸ¤ Contributi

Questo Ã¨ un progetto di ricerca sperimentale. Idee:

1. Nuovi operatori booleani
2. Strategie evolutive avanzate (coevolution, novelty search)
3. Visualizzazione lattice durante evoluzione
4. Hardware acceleration (FPGA implementation)
5. Benchmark rigorosi vs state-of-the-art

## ğŸ“„ Licenza

[Specifica licenza]

## ğŸ™ Riconoscimenti

Ispirato da:
- Cellular automata (Conway, Wolfram)
- Genetic programming (Koza)
- Evolutionary algorithms (Holland)
- Biological neural computation

---

**EVE**: L'AI che evolve invece di backpropagare.
